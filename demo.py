#!/usr/bin/env python3
"""
046211 Deep Learning Project - Personal Food Taste Classification Demo

Quick demonstration script for the personal food taste classification system.
This script provides an interactive way to test the trained models.

Author: Noam Shani
Course: 046211 Deep Learning, Technion

Attribution:
- Model architecture: Custom EfficientNet-B3 based classifier
- EfficientNet: Tan, M., & Le, Q. (2019). EfficientNet: Rethinking model scaling. ICML.
- timm library: https://github.com/rwightman/pytorch-image-models
- PyTorch: https://pytorch.org/
"""

import torch
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
import json
import argparse
from pathlib import Path

# Import our modules
from src.model import create_model

class PersonalTasteDemo:
    """Interactive demo for personal food taste classification."""
    
    def __init__(self, model_path=None):
        """Initialize the demo with a trained model."""
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.class_names = {0: 'disgusting', 1: 'neutral', 2: 'tasty'}
        
        # Default to best model if not specified
        if model_path is None:
            model_path = "experiments/personal_60_20_20/best_model.pt"
        
        self.model = self.load_model(model_path)
        self.transform = self.get_transforms()
        
        print(f"ğŸ½ï¸ Personal Food Taste Classifier Demo")
        print(f"Device: {self.device}")
        print(f"Model: {model_path}")
        print("="*50)
    
    def get_transforms(self):
        """Get image preprocessing transforms."""
        return transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    
    def load_model(self, model_path):
        """Load the trained model."""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # Get model config
            model_config = checkpoint.get('config', {}).get('model', {
                'num_classes': 3,
                'backbone': 'tf_efficientnet_b3',
                'pretrained': True,
                'dropout': 0.3,
                'hidden_dim': 256
            })
            
            # Create and load model
            model = create_model(model_config, self.device)
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()
            
            print(f"âœ… Model loaded successfully!")
            return model
            
        except Exception as e:
            print(f"âŒ Error loading model: {e}")
            print("Available models:")
            print("  - experiments/personal_60_20_20/best_model.pt (recommended)")
            print("  - experiments/personal_80_20/best_model.pt")
            print("  - experiments/base_model/best_model.pt")
            raise
    
    def predict_image(self, image_path):
        """Predict taste preference for a food image."""
        try:
            # Load and preprocess image
            image = Image.open(image_path).convert('RGB')
            input_tensor = self.transform(image).unsqueeze(0).to(self.device)
            
            # Make prediction
            with torch.no_grad():
                outputs = self.model(input_tensor)
                probabilities = F.softmax(outputs, dim=1)
                predicted_class = torch.argmax(outputs, dim=1).item()
            
            # Format results
            results = {
                'predicted_class': predicted_class,
                'predicted_label': self.class_names[predicted_class],
                'confidence': probabilities[0][predicted_class].item(),
                'all_probabilities': {
                    self.class_names[i]: probabilities[0][i].item() 
                    for i in range(3)
                }
            }
            
            return results
            
        except Exception as e:
            print(f"âŒ Error processing image: {e}")
            return None
    
    def print_results(self, results, image_path):
        """Print prediction results in a nice format."""
        if results is None:
            return
        
        print(f"\nğŸ“¸ Image: {image_path}")
        print(f"ğŸ¯ Prediction: {results['predicted_label'].upper()}")
        print(f"ğŸª Confidence: {results['confidence']:.1%}")
        print(f"\nğŸ“Š All Probabilities:")
        
        for label, prob in results['all_probabilities'].items():
            bar_length = int(prob * 20)  # Scale to 20 chars
            bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
            print(f"  {label:>10}: {bar} {prob:.1%}")
        
        # Interpretation
        print(f"\nğŸ’­ Interpretation:")
        if results['confidence'] > 0.7:
            print(f"   High confidence - the model is quite sure this food is {results['predicted_label']}")
        elif results['confidence'] > 0.5:
            print(f"   Moderate confidence - the model thinks this food is {results['predicted_label']}")
        else:
            print(f"   Low confidence - the model is uncertain about this prediction")
    
    def interactive_mode(self):
        """Run interactive demo mode."""
        print(f"\nğŸ® Interactive Mode")
        print(f"Enter image paths to classify (type 'quit' to exit)")
        print(f"Example: experiments/base_model/sample_images/pizza.jpg")
        
        while True:
            image_path = input(f"\nğŸ“ Image path: ").strip()
            
            if image_path.lower() in ['quit', 'exit', 'q']:
                print(f"ğŸ‘‹ Thanks for using the demo!")
                break
            
            if not Path(image_path).exists():
                print(f"âŒ File not found: {image_path}")
                continue
            
            results = self.predict_image(image_path)
            self.print_results(results, image_path)

def main():
    """Main demo function."""
    parser = argparse.ArgumentParser(description='Personal Food Taste Classification Demo')
    parser.add_argument('--image', type=str, help='Path to food image to classify')
    parser.add_argument('--model', type=str, help='Path to model checkpoint')
    parser.add_argument('--interactive', action='store_true', help='Run in interactive mode')
    
    args = parser.parse_args()
    
    try:
        # Initialize demo
        demo = PersonalTasteDemo(model_path=args.model)
        
        if args.image:
            # Single image prediction
            results = demo.predict_image(args.image)
            demo.print_results(results, args.image)
            
        elif args.interactive:
            # Interactive mode
            demo.interactive_mode()
            
        else:
            # Default: show demo info and run interactive
            print(f"\nğŸ¯ Available Models:")
            print(f"  1. Personal 60/20/20 Split (Recommended): experiments/personal_60_20_20/best_model.pt")
            print(f"  2. Personal 80/20 Split: experiments/personal_80_20/best_model.pt") 
            print(f"  3. Base Food101 Model: experiments/base_model/best_model.pt")
            
            print(f"\nğŸš€ Usage Examples:")
            print(f"  python demo.py --image path/to/food/image.jpg")
            print(f"  python demo.py --interactive")
            print(f"  python demo.py --image pizza.jpg --model experiments/personal_80_20/best_model.pt")
            
            # Ask user if they want to run interactive mode
            response = input(f"\nRun interactive mode? (y/n): ").strip().lower()
            if response in ['y', 'yes']:
                demo.interactive_mode()
                
    except KeyboardInterrupt:
        print(f"\nğŸ‘‹ Demo interrupted by user")
    except Exception as e:
        print(f"âŒ Demo error: {e}")

if __name__ == "__main__":
    main()
